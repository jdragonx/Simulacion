{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b45903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24eab88",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = int(2e5) #cantidad de filas en los datos a ser generados\n",
    "nit = 300 #número de instantes de tiempo que se intentará reproducir\n",
    "files = os.listdir('Data') #listado de todos los archivos de texto generados\n",
    "start = -1 #Inicio de los datos para la interpolación\n",
    "end = 1 #Fin de los datos para la interpolación\n",
    "ndiv = int(nrows**(1/3)) #Número de divisiones en cada eje, por eso se obtiene de la raíz cúbica y se trunca\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5296ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = [file[12:-4].split('_') for file in files] #Obtención de los puntos por cada nombre de archivo de datos\n",
    "p = np.array(p) #Transformación de los puntos a numpy ndarray\n",
    "\n",
    "#Carga de cada dato, se realiza la transposición para convertir en fila y se suma los valores de transmisión directa y difusa\n",
    "df = pd.DataFrame([pd.read_csv('Data/'+file,header=None,sep='\\t').T.sum(axis=0).values for file in files])\n",
    "\n",
    "df_p = pd.DataFrame() #Dataframe vacío para cargar los puntos obtenidos anteriormente\n",
    "df_p['px'], df_p['py'], df_p['pz'] = p[:,0], p[:,1], p[:,2] #Carga de los puntos en el dataframe\n",
    "df_p = df_p.astype('float64') #Transformación de los puntos a flotante\n",
    "df_p = df_p.values #Obtención de los puntos en forma de numpy ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a34052b",
   "metadata": {},
   "outputs": [],
   "source": [
    "coor = np.linspace(start,end,ndiv) #Coordenadas para la división que se va a realizar en cada eje, espacios equidistantes\n",
    "new_points = np.array(np.meshgrid(coor,coor,coor)).T.reshape(-1,3) #Creación de los nuevos puntos con todas las combinaciones posibles y transposición\n",
    "nrows = new_points.shape[0] #Obtención de la verdadera cantidad de filas que se van a crear\n",
    "nrows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8ad6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtención de las distancias de cada uno de los nuevos puntos a los puntos anteriores, se repite los nuevos puntos y se forma \n",
    "#2 matrices, una con los nuevos puntos y la otra con los puntos originales para luego realizar una resta completa y obtener una\n",
    "#matriz de distancias de cada eje de cada punto nuevo hacia los puntos originales\n",
    "dist = np.abs(np.tile(new_points,27)-np.repeat(df_p.reshape(1,-1),nrows,axis=0))\n",
    "\n",
    "dist_sel = np.round(dist,10) #Redondeo de las distancias obtenidas para corregir valores de 0, 1 y 2\n",
    "\n",
    "#Selección de todas las filas, y las columnas de 3 en 3 para selecciones por separado las distancias que son mayores a 1 en cada\n",
    "#eje, así podemos evitar que se hagan interpolaciones con valores que estén muy alejados\n",
    "ds = (dist_sel[:,::3]>1).astype('int')+(dist_sel[:,1::3]>1).astype('int')+(dist_sel[:,2::3]>1).astype('int')\n",
    "\n",
    "#Se ordena de menor a mayor, dado que se transformó el booleano en entero se puede utilizar esto para tomar los valores de 0\n",
    "#como aquellas distancias a los puntos más cercanos\n",
    "cls = np.argsort(ds,axis=1)[:,:8] #argsort devuelve los índices del nuevo orden\n",
    "\n",
    "distances = pd.DataFrame() #Se crea un dataframe para almacenar las distancias finales que se usarán para la interpolación\n",
    "\n",
    "#Selección de las distancias más cercanas, asignándolas a cada nuevo punto según su id y usando selección condicional\n",
    "for i in range(0,24,3):\n",
    "    distances[str(i)]=dist[:,::3][np.arange(nrows), cls[:,int(i/3)]]\n",
    "    distances[str(i+1)]=dist[:,1::3][np.arange(nrows), cls[:,int(i/3)]]\n",
    "    distances[str(i+2)]=dist[:,2::3][np.arange(nrows), cls[:,int(i/3)]]\n",
    "mult = (1-distances.abs()).values #Se usa el valor de 1 menos las distancias para la multiplicación de la interpolación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84461b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = pd.DataFrame() #Se crea un dataframe para los valores que se usarán en las interpolaciones\n",
    "for i in range(8):\n",
    "    \n",
    "    #Se obtiene el valor correspondiente a los 8 puntos más cercanos y en cada punto se multiplican los valores de 'x', 'y', 'z'\n",
    "    val[str(i)] = np.prod(mult[:,i*3:i*3+3],axis=1) \n",
    "    \n",
    "val = np.repeat(val.values,nit,axis=1) #Se repite el valor del porcentaje de cada punto el número de veces necesaria\n",
    "cls = pd.DataFrame(cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2e09e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ca24b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se define una función que creará fila a fila el dataframe final compuesto de los valores interpolados\n",
    "def getBigDf(x):\n",
    "    \n",
    "    #Se recibe el índice, se extraen los valores que le corresponden y se multiplica por los datos según el índice dado por la \n",
    "    #fila de la variable cls, se recorre hasta los instantes de tiempo acordados\n",
    "    minidf = val[x.name] * df.iloc[x,:nit].values.reshape(1,-1)[0] \n",
    "    \n",
    "    retorno = [np.sum(minidf[i::nit]) for i in range(nit)] #Se suman los valores de los 3 ejes para cada instante\n",
    "    return retorno\n",
    "\n",
    "#Se aplica la función definida anteriormente a cada fila y se expande el resultado que llega a modo de lista\n",
    "bigdf = pd.DataFrame(cls.apply(lambda x: getBigDf(x),axis=1,result_type='expand'))\n",
    "\n",
    "points = pd.DataFrame(new_points) #Se crea un dataframe con los nuevos puntos que fueron creados\n",
    "points.columns = ['x','y','z']\n",
    "\n",
    "#Se guarda los datos creados por la interpolación para no tener que crearlos nuevamente\n",
    "bigdf.to_csv('targets.csv',header=False,index=False,chunksize=10000)\n",
    "points.to_csv('points.csv',header=False,index=False,chunksize=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4cf9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ahora ya se puede cargar los datos creados por la interpolación\n",
    "bigdf = pd.read_csv('targets.csv',header=None)\n",
    "points = pd.read_csv('points.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce27bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b62a4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b00448",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se define una función para graficar el comportamiento de la energía\n",
    "def graph(x):\n",
    "    clear_output(wait=True)\n",
    "    plt.plot(x.index, x)\n",
    "    coo = points.iloc[x.name].values\n",
    "    plt.title(\"x: {}, y: {}, z:{}\".format(coo[0],coo[1],coo[2]))\n",
    "    plt.show()\n",
    "\n",
    "#Se procede a graficar el comportamiento de la energía de los nuevos puntos para una verificación visual\n",
    "for i in range(1):\n",
    "    graph(bigdf.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4216141",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eba098c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e091ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d945c120",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se separa los datos en 3 sets usando una semilla para asegurar que se separen en el mismo orden\n",
    "x_train, x_test = train_test_split(points, train_size=0.85, random_state=5, shuffle=True)\n",
    "x_val, x_test = train_test_split(x_test, train_size=0.5, random_state=5, shuffle=True)\n",
    "\n",
    "y_train, y_test = train_test_split(bigdf, train_size=0.85, random_state=5, shuffle=True)\n",
    "y_val, y_test = train_test_split(y_test, train_size=0.5, random_state=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b94a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se pone una semilla a las librerías que se usará para el deep learning\n",
    "np.random.seed(5)\n",
    "tf.random.set_seed(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f503a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se crea el perceptrón usando la api funcional de keras\n",
    "inp = keras.layers.Input(points.shape[1],name='entrada') #Se crea la entrada\n",
    "\n",
    "#Dado que la función de activación tanh puede dar valores que crecen indefinidamente, se usa sigmoides después\n",
    "#Así se controla que los datos no tiendan a infinito fácilmente\n",
    "x = keras.layers.Dense(15,activation='tanh',name='capa1')(inp) #primera capa incrementando la dimensión de 3 a 15\n",
    "x = keras.layers.Dense(35,activation='softplus',name='capa2')(x) #segunda capa incrementando a 35 la dimensión\n",
    "x = keras.layers.Dense(75,activation='sigmoid',name='capa3')(x) #tercera capa incrementando a 75 la activación\n",
    "x = keras.layers.Dense(150,activation='tanh',name='capa4')(x) #cuarta capa incrementando a 150 la activación\n",
    "\n",
    "#Para salida se usa relu, dado que los valores están entre 0 y 1\n",
    "x = keras.layers.Dense(bigdf.shape[1],activation='relu',name='salida')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d792e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se crea el modelo indicando entradas y salidas\n",
    "model = keras.Model(\n",
    "    inputs = inp,\n",
    "    outputs = x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b4cbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se compila el modelo con optimizador de rmsprop, que mantiene una media móvil del cuadrado de los gradientes y\n",
    "#luego divide el nuevo gradiente para la raíz de esta media. La ventaja de este algoritmo es que automáticamente reduce\n",
    "#el tamaño de los pasos que se toman a medida que se aproxima al mínimo. Se utiliza el error medio cuadrático para medir\n",
    "#la pérdida y una métrica de precisión al final que se obtiene al redondear al valor más cercano.\n",
    "model.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='mse',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c51988f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se entrena el modelo con 1000 epochs y un early stopping en 100 epochs, con la capacidad de restaurar los mejores pesos\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=1000,\n",
    "    batch_size=1024,\n",
    "    verbose=0,\n",
    "    callbacks=[callbacks.EarlyStopping(monitor='val_loss', patience=100, min_delta=0, restore_best_weights=True)],\n",
    "    validation_data=(x_val,y_val) #Se va validando el entrenamiento constantemente\n",
    ")\n",
    "model.save('perceptron.h5') #Se guarda el modelo para no tener que entrenarlo de nuevo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e554e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(history.history,open('history.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc41004",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = pickle.load(open('history.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785a1edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se carga el modelo para realizar las pruebas\n",
    "model = keras.models.load_model('perceptron.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3745769c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se grafica el avance en la medida de loss con los datos de entrenamiento y validación\n",
    "plt.plot(history['loss'])\n",
    "plt.plot(history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c792e598",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se grafica la mejora en el accuracy mientras entrena con los datos de entrenamiento y validación\n",
    "plt.plot(history['accuracy'])\n",
    "plt.plot(history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2357a983",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se realiza una predicción con la tercera porción de datos nunca vista por el modelo\n",
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bc46d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se crea una función para graficar rápidamente los datos reales vs los datos predichos en la prueba\n",
    "def graph_double(x1,x2):\n",
    "    clear_output(wait=True)\n",
    "    plt.plot(x1.index, x1)\n",
    "    plt.plot(x1.index, x2)\n",
    "    plt.legend(['real', 'pred'])\n",
    "    coo = points.iloc[x1.name].values\n",
    "    plt.title(\"x: {}, y: {}, z:{}\".format(coo[0],coo[1],coo[2]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9390f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se realiza una evaluación que arroja 95% de accuracy con los datos de prueba que nunca fueron vistos\n",
    "model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787c32c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se visualiza la comparación entre los datos de prueba y la predicción del modelo\n",
    "for i in range(1):\n",
    "    graph_double(y_test.iloc[i],y_pred[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77389b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se obtienen las coordenadas de los puntos en los ejes\n",
    "un_points = points[2].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e62adfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se evalúa el modelo con la totalidad de los datos\n",
    "model.evaluate(points,bigdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644af08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_y = model.predict(points) #Se realiza una predicción de la totalidad de los datos\n",
    "\n",
    "#Se obtiene una medida de error medio cuadrático para cada punto tomando en cuenta todos los instantes de tiempo\n",
    "se = ((total_y - bigdf)**2)\n",
    "mse = se.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d9b619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f8d8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se grafica un mapa de calor del error medio cuadrático para cada nivel del eje z, en el plano x,y\n",
    "for z in un_points:\n",
    "    plotMx = np.zeros((len(un_points),len(un_points)))\n",
    "    for i,x in enumerate(un_points):\n",
    "        for j,y in enumerate(un_points):\n",
    "            plotMx[-j,i] = mse[points[2] == z][points[0] == x][points[1] == y]\n",
    "    plotDf = pd.DataFrame(plotMx)\n",
    "    plotDf.columns = un_points\n",
    "    plotDf.index = un_points*-1\n",
    "    ax = sns.heatmap(plotDf,vmin=0, vmax=1e-5)\n",
    "    ax.set_title(\"Error medio cuadrático en el plano x,y para z={}\".format(z))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b70fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se visualizan los puntos originales en un gráfico\n",
    "sel = (df_p[:,0] == 0.) * (df_p[:,1] == 0.) * (df_p[:,2] == 0.)\n",
    "\n",
    "x1 = df.iloc[:,:300][sel].iloc[0]\n",
    "x2 = model.predict(df_p)[sel].reshape(-1,1)\n",
    "clear_output(wait=True)\n",
    "plt.plot(x1.index, x1)\n",
    "plt.plot(x1.index, x2)\n",
    "plt.legend(['real', 'pred'])\n",
    "coo = df_p[x1.name]\n",
    "plt.title(\"x: {}, y: {}, z:{}\".format(coo[0],coo[1],coo[2]))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
